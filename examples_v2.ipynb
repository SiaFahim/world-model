{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["2022-12-20 13:35:35.137526: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n", "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}], "source": "import numpy as np\nimport tensorflow as tf\nfrom collections import namedtuple\n\n# Setting the Hyperparameters\nHyperParams = namedtuple('HyperParams', ['num_steps',\n                                         'max_seq_len',\n                                         'input_seq_width',\n                                         'output_seq_width',\n                                         'rnn_size',\n                                         'batch_size',\n                                         'grad_clip',\n                                         'num_mixture',\n                                         'learning_rate',\n                                         'decay_rate',\n                                         'min_learning_rate',\n                                         'use_layer_norm',\n                                         'use_recurrent_dropout',\n                                         'recurrent_dropout_prob',\n                                         'use_input_dropout',\n                                         'input_dropout_prob',\n                                         'use_output_dropout',\n                                         'output_dropout_prob',\n                                         'is_training',\n                                         ])\n\n# Making a function the returns all the values of the default hyperparameters\ndef get_default_hps():\n    return HyperParams(num_steps=2000,\n                       max_seq_len=1000,\n                       input_seq_width=35,\n                       output_seq_width=32,\n                       rnn_size=256,\n                       batch_size=100,\n                       grad_clip= 1.0,\n                       num_mixture=5,\n                       learning_rate=0.001,\n                       decay_rate=1.0,\n                       min_learning_rate=0.00001,\n                       use_layer_norm=0,\n                       use_recurrent_dropout=0,\n                       recurrent_dropout_prob=0.90,\n                       use_input_dropout=0,\n                       input_dropout_prob=0.90,\n                       use_output_dropout=0,\n                       output_dropout_prob=0.90,\n                       is_training=1,\n                       )\n\n# Getting these default hyperparameters\nhps = get_default_hps()"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["2022-12-20 13:35:45.669751: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n", "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}, {"ename": "AttributeError", "evalue": "module 'tensorflow' has no attribute 'contrib'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mif\u001b[39;00m hps\u001b[39m.\u001b[39mis_training:\n\u001b[1;32m      7\u001b[0m     global_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(\u001b[39m0\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mglobal_step\u001b[39m\u001b[39m'\u001b[39m, trainable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m cell_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcontrib\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mLayerNormBasicLSTMCell\n\u001b[1;32m      9\u001b[0m use_recurrent_dropout \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m hps\u001b[39m.\u001b[39muse_recurrent_dropout \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m use_input_dropout \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m hps\u001b[39m.\u001b[39muse_input_dropout \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n", "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"]}], "source": "# Building the RNN\nKMIX = hps.num_mixture\nINWIDTH = hps.input_seq_width\nOUTWIDTH = hps.output_seq_width\nLENGTH = hps.max_seq_len\nif hps.is_training:\n    global_step = tf.Variable(0, name='global_step', trainable=False)\ncell_fn = tf.contrib.rnn.LayerNormBasicLSTMCell\nuse_recurrent_dropout = False if hps.use_recurrent_dropout == 0 else True\nuse_input_dropout = False if hps.use_input_dropout == 0 else True\nuse_output_dropout = False if hps.use_output_dropout == 0 else True\nuse_layer_norm = False if hps.use_layer_norm == 0 else True\nif use_recurrent_dropout:\n    cell = cell_fn(hps.rnn_size, layer_norm=use_layer_norm, dropout_keep_prob=hps.recurrent_dropout_prob)\nelse:\n    cell = cell_fn(hps.rnn_size, layer_norm=use_layer_norm)\nif use_input_dropout:\n    cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=hps.input_dropout_prob)\nif use_output_dropout:\n    cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=hps.output_dropout_prob)\nsequence_lengths = LENGTH\ninput_x = tf.compat.v1.placeholder(tf.float32, shape=[hps.batch_size, hps.max_seq_len, INWIDTH], name='input_x')\noutput_x = tf.compat.v1.placeholder(tf.float32, shape=[hps.batch_size, hps.max_seq_len, OUTWIDTH], name='output_x')\nactual_input = input_x\ninitial_state = cell.zero_state(batch_size=hps.batch_size, dtype=tf.float32)\nNOUT = OUTWIDTH * KMIX * 3\nwith tf.compat.v1.variable_scope('RNN'):\n    output_w = tf.compat.v1.get_variable(\"output_w\", [hps.rnn_size, NOUT]) # Output weights\n    output_b = tf.compat.v1.get_variable(\"output_b\", [NOUT]) # Output biases\noutput, last_state = tf.compat.v1.nn.dynamic_rnn(cell=cell,\n                                        inputs=actual_input,\n                                        initial_state=initial_state,\n                                        dtype=tf.float32,\n                                        swap_memory=True,\n                                        scope='RNN')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "print(tf.__version__)"}], "metadata": {"kernelspec": {"display_name": "ai", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}, "orig_nbformat": 4, "vscode": {"interpreter": {"hash": "980b38fd3b64562c66d89b44e61313b9d67bf24d89ee15f18c143a9e514ee2e5"}}}, "nbformat": 4, "nbformat_minor": 2}