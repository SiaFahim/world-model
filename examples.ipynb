{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 13:35:35.137526: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple\n",
    "\n",
    "# Setting the Hyperparameters\n",
    "HyperParams = namedtuple('HyperParams', ['num_steps',\n",
    "                                         'max_seq_len',\n",
    "                                         'input_seq_width',\n",
    "                                         'output_seq_width',\n",
    "                                         'rnn_size',\n",
    "                                         'batch_size',\n",
    "                                         'grad_clip',\n",
    "                                         'num_mixture',\n",
    "                                         'learning_rate',\n",
    "                                         'decay_rate',\n",
    "                                         'min_learning_rate',\n",
    "                                         'use_layer_norm',\n",
    "                                         'use_recurrent_dropout',\n",
    "                                         'recurrent_dropout_prob',\n",
    "                                         'use_input_dropout',\n",
    "                                         'input_dropout_prob',\n",
    "                                         'use_output_dropout',\n",
    "                                         'output_dropout_prob',\n",
    "                                         'is_training',\n",
    "                                         ])\n",
    "\n",
    "# Making a function the returns all the values of the default hyperparameters\n",
    "def get_default_hps():\n",
    "    return HyperParams(num_steps=2000,\n",
    "                       max_seq_len=1000,\n",
    "                       input_seq_width=35,\n",
    "                       output_seq_width=32,\n",
    "                       rnn_size=256,\n",
    "                       batch_size=100,\n",
    "                       grad_clip= 1.0,\n",
    "                       num_mixture=5,\n",
    "                       learning_rate=0.001,\n",
    "                       decay_rate=1.0,\n",
    "                       min_learning_rate=0.00001,\n",
    "                       use_layer_norm=0,\n",
    "                       use_recurrent_dropout=0,\n",
    "                       recurrent_dropout_prob=0.90,\n",
    "                       use_input_dropout=0,\n",
    "                       input_dropout_prob=0.90,\n",
    "                       use_output_dropout=0,\n",
    "                       output_dropout_prob=0.90,\n",
    "                       is_training=1,\n",
    "                       )\n",
    "\n",
    "# Getting these default hyperparameters\n",
    "hps = get_default_hps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 13:35:45.669751: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mif\u001b[39;00m hps\u001b[39m.\u001b[39mis_training:\n\u001b[1;32m      7\u001b[0m     global_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(\u001b[39m0\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mglobal_step\u001b[39m\u001b[39m'\u001b[39m, trainable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m cell_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcontrib\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mLayerNormBasicLSTMCell\n\u001b[1;32m      9\u001b[0m use_recurrent_dropout \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m hps\u001b[39m.\u001b[39muse_recurrent_dropout \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m use_input_dropout \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m hps\u001b[39m.\u001b[39muse_input_dropout \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "# Building the RNN\n",
    "KMIX = hps.num_mixture\n",
    "INWIDTH = hps.input_seq_width\n",
    "OUTWIDTH = hps.output_seq_width\n",
    "LENGTH = hps.max_seq_len\n",
    "if hps.is_training:\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "cell_fn = tf.contrib.rnn.LayerNormBasicLSTMCell\n",
    "use_recurrent_dropout = False if hps.use_recurrent_dropout == 0 else True\n",
    "use_input_dropout = False if hps.use_input_dropout == 0 else True\n",
    "use_output_dropout = False if hps.use_output_dropout == 0 else True\n",
    "use_layer_norm = False if hps.use_layer_norm == 0 else True\n",
    "if use_recurrent_dropout:\n",
    "    cell = cell_fn(hps.rnn_size, layer_norm=use_layer_norm, dropout_keep_prob=hps.recurrent_dropout_prob)\n",
    "else:\n",
    "    cell = cell_fn(hps.rnn_size, layer_norm=use_layer_norm)\n",
    "if use_input_dropout:\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=hps.input_dropout_prob)\n",
    "if use_output_dropout:\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=hps.output_dropout_prob)\n",
    "sequence_lengths = LENGTH\n",
    "input_x = tf.placeholder(tf.float32, shape=[hps.batch_size, hps.max_seq_len, INWIDTH], name='input_x')\n",
    "output_x = tf.placeholder(tf.float32, shape=[hps.batch_size, hps.max_seq_len, OUTWIDTH], name='output_x')\n",
    "actual_input = input_x\n",
    "initial_state = cell.zero_state(batch_size=hps.batch_size, dtype=tf.float32)\n",
    "NOUT = OUTWIDTH * KMIX * 3\n",
    "with tf.variable_scope('RNN'):\n",
    "    output_w = tf.get_variable(\"output_w\", [hps.rnn_size, NOUT]) # Output weights\n",
    "    output_b = tf.get_variable(\"output_b\", [NOUT]) # Output biases\n",
    "output, last_state = tf.nn.dynamic_rnn(cell=cell,\n",
    "                                        inputs=actual_input,\n",
    "                                        initial_state=initial_state,\n",
    "                                        dtype=tf.float32,\n",
    "                                        swap_memory=True,\n",
    "                                        scope='RNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-20 13:44:17.321246: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Extracting code lines from original notebook\n",
      "WARNING line 59:10: tf.contrib.rnn.LayerNormBasicLSTMCell requires manual check. (Manual edit required) `tf.contrib.rnn.LayerNormBasicLSTMCell` has been migrated to `tfa.rnn.LayerNormLSTMCell` in TensorFlow Addons. The API spec may have changed during the migration. Please see https://github.com/tensorflow/addons for more info.\n",
      "WARNING line 59:10: Using member tf.contrib.rnn.LayerNormBasicLSTMCell in deprecated module tf.contrib.rnn. (Manual edit required) tf.contrib.rnn.* has been deprecated, and widely used cells/functions will be moved to tensorflow/addons repository. Please check it there and file Github issues if necessary.\n",
      "ERROR line 59:10: Using member tf.contrib.rnn.LayerNormBasicLSTMCell in deprecated module tf.contrib. tf.contrib.rnn.LayerNormBasicLSTMCell cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n",
      "INFO line 69:11: Renamed 'tf.nn.rnn_cell.DropoutWrapper' to 'tf.compat.v1.nn.rnn_cell.DropoutWrapper'\n",
      "INFO line 71:11: Renamed 'tf.nn.rnn_cell.DropoutWrapper' to 'tf.compat.v1.nn.rnn_cell.DropoutWrapper'\n",
      "INFO line 73:10: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
      "INFO line 74:11: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
      "INFO line 78:5: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
      "WARNING line 79:15: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
      "INFO line 79:15: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
      "WARNING line 80:15: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
      "INFO line 80:15: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
      "INFO line 81:21: Renamed 'tf.nn.dynamic_rnn' to 'tf.compat.v1.nn.dynamic_rnn'\n",
      "TensorFlow 2.0 Upgrade Script\n",
      "-----------------------------\n",
      "Converted 1 files\n",
      "Detected 5 issues that require attention\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "File: examples.ipynb\n",
      "--------------------------------------------------------------------------------\n",
      "examples.ipynb:59:10: WARNING: tf.contrib.rnn.LayerNormBasicLSTMCell requires manual check. (Manual edit required) `tf.contrib.rnn.LayerNormBasicLSTMCell` has been migrated to `tfa.rnn.LayerNormLSTMCell` in TensorFlow Addons. The API spec may have changed during the migration. Please see https://github.com/tensorflow/addons for more info.\n",
      "examples.ipynb:59:10: WARNING: Using member tf.contrib.rnn.LayerNormBasicLSTMCell in deprecated module tf.contrib.rnn. (Manual edit required) tf.contrib.rnn.* has been deprecated, and widely used cells/functions will be moved to tensorflow/addons repository. Please check it there and file Github issues if necessary.\n",
      "examples.ipynb:59:10: ERROR: Using member tf.contrib.rnn.LayerNormBasicLSTMCell in deprecated module tf.contrib. tf.contrib.rnn.LayerNormBasicLSTMCell cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n",
      "examples.ipynb:79:15: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
      "examples.ipynb:80:15: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
      "\n",
      "\n",
      "Make sure to read the detailed log 'report.txt'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tf_upgrade_v2 --infile examples.ipynb --outfile examples_v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "980b38fd3b64562c66d89b44e61313b9d67bf24d89ee15f18c143a9e514ee2e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
